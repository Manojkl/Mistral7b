{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T17:40:02.518609Z","iopub.status.busy":"2023-12-13T17:40:02.517666Z","iopub.status.idle":"2023-12-13T17:40:09.648263Z","shell.execute_reply":"2023-12-13T17:40:09.646763Z","shell.execute_reply.started":"2023-12-13T17:40:02.518556Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer\n","\n","\n","def get_distribution(logits, temperature):\n","    probs = torch.softmax(logits / (temperature + 1e-10), dim=-1)\n","    return probs\n","\n","\n","def sample(logits, temperature):\n","    probs = get_distribution(logits, temperature)\n","    return torch.multinomial(probs, num_samples=1)[0]\n","\n","\n","def sample_from_draft_model(model, initial_prompt_seq, new_tokens, temperature=1.0):\n","    fin_prompt_seq = initial_prompt_seq.detach().clone()\n","    out_logits = []\n","\n","    for _ in range(new_tokens):\n","        sample_token_logits = model(fin_prompt_seq).logits[:, -1, :]\n","        sample_token = sample(sample_token_logits, temperature=temperature)\n","        fin_prompt_seq = torch.concat([fin_prompt_seq, sample_token[None, ...]], dim=-1)\n","        out_logits.append(sample_token_logits)\n","\n","    out_logits = torch.stack(out_logits, dim=1)\n","    return fin_prompt_seq, out_logits\n","\n","\n","def autoregressive_sampling(model, initial_prompt_seq, target_len, temperature=1.0):\n","    n = initial_prompt_seq.shape[-1]\n","    fin_prompt_seq = initial_prompt_seq.detach().clone()\n","\n","    while n < target_len:\n","        sample_token_logits = model(fin_prompt_seq).logits[:, -1, :]\n","        sample_token = sample(sample_token_logits, temperature=temperature)\n","        fin_prompt_seq = torch.concat([fin_prompt_seq, sample_token[None, ...]], dim=-1)\n","        n += 1\n","    return fin_prompt_seq\n","\n","\n","def speculative_sampling(\n","    target_model,\n","    draft_model,\n","    initial_prompt_seq,\n","    max_new_tokens,\n","    tokenizer,\n","    lookahead=4,\n","    temperature=1.0,\n","    debug=True,\n","):\n","    \"\"\"\n","    Implementation of Algorithm 2 of the paper - Accelerating Large Language Model Decoding\n","    with Speculative Sampling (https://arxiv.org/abs/2302.01318)\n","    \"\"\"\n","    assert initial_prompt_seq.shape[0] == 1, \"Batch size should be 1\"\n","\n","    n = initial_prompt_seq.shape[-1]\n","    fin_prompt_seq = initial_prompt_seq.detach().clone()\n","\n","    while n < max_new_tokens:\n","        n_orig = n\n","        N = fin_prompt_seq.shape[-1]\n","        draft_outputs, draft_logits = sample_from_draft_model(\n","            draft_model, fin_prompt_seq, new_tokens=lookahead, temperature=temperature\n","        )\n","\n","        if debug:\n","            print(\n","                f\"Possible continuations: {tokenizer.decode(draft_outputs[0,n_orig:], skip_special_tokens=True)}\"\n","            )\n","\n","        target_logits = target_model(draft_outputs).logits[:, -lookahead - 1 :, :]\n","\n","        target_model_distribution = get_distribution(target_logits, temperature)\n","        draft_model_distribution = get_distribution(draft_logits, temperature)\n","\n","        accepted_flag = 1\n","\n","        for t in range(lookahead):\n","            numerator = target_model_distribution[:, t, draft_outputs[0, N + t]]\n","            denominator = draft_model_distribution[:, t, draft_outputs[0, N + t]]\n","            ratio = numerator / denominator\n","            uniform_distribution = torch.rand_like(numerator)\n","            ones_tensor = torch.ones_like(numerator)\n","\n","            # Rejection Sampling\n","            ## Acceptance\n","            if (uniform_distribution < torch.min(ones_tensor, ratio)).any():\n","                fin_prompt_seq = torch.concat(\n","                    [fin_prompt_seq, draft_outputs[:, N + t].unsqueeze(dim=-1)], dim=-1\n","                )\n","                n += 1\n","\n","            ## Rejection\n","            else:\n","                new_dist = (\n","                    target_model_distribution[:, t, :]\n","                    - draft_model_distribution[:, t, :]\n","                )\n","                new_dist = torch.max(torch.zeros_like(new_dist), new_dist)\n","                new_dist = new_dist / new_dist.sum(dim=-1, keepdim=True)\n","                token_id = torch.multinomial(new_dist, num_samples=1)[0]\n","                fin_prompt_seq = torch.concat(\n","                    [fin_prompt_seq, token_id[None, ...]], dim=-1\n","                )\n","                accepted_flag = 0\n","                break\n","\n","        if accepted_flag == 1:\n","            sample_token = sample(target_logits[:, -1, :], temperature=temperature)\n","            fin_prompt_seq = torch.concat(\n","                [fin_prompt_seq, sample_token[None, ...]], dim=-1\n","            )\n","\n","        if debug:\n","            print(\n","                f\"Accepted continuations: {tokenizer.decode(fin_prompt_seq[0,n_orig:], skip_special_tokens=True)}\"\n","            )\n","\n","        n += 1\n","\n","    return fin_prompt_seq"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T17:38:10.790878Z","iopub.status.busy":"2023-12-13T17:38:10.790242Z","iopub.status.idle":"2023-12-13T17:39:27.159720Z","shell.execute_reply":"2023-12-13T17:39:27.158615Z","shell.execute_reply.started":"2023-12-13T17:38:10.790825Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d57dd26468894371a76273c524efe29c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["time_taken speculative: 0.001486268\n","<s> Emily found a mysterious letter on her doorstep one sunny morning.\n","\n","Latency (Speculative Sampling): 9419.57 tok/s\n"]}],"source":["import sys\n","import time\n","import random\n","import argparse\n","import torch\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n","print(device)\n","\n","method = \"speculative\"\n","target_model_1 = \"mistralai/Mistral-7B-Instruct-v0.1\"\n","draft_model = \"EleutherAI/pythia-160m-deduped\"\n","prompt = \"Emily found a mysterious letter on her doorstep one sunny morning.\"\n","max_new_tokens_1 = 12\n","temperature_1 = 1\n","\n","if method == \"speculative\":\n","\n","    target_model = AutoModelForCausalLM.from_pretrained(target_model_1).to(device)\n","    draft_model = AutoModelForCausalLM.from_pretrained(draft_model).to(device)\n","    tokenizer = AutoTokenizer.from_pretrained(target_model_1)\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    start_time = time.time_ns()\n","    tokens = speculative_sampling(\n","        target_model,\n","        draft_model,\n","        initial_prompt_seq=inputs.input_ids,\n","        max_new_tokens=max_new_tokens_1,\n","        tokenizer=tokenizer,\n","        temperature=temperature_1,\n","        debug=False,\n","    )\n","    end_time = time.time_ns()\n","\n","    new_tokens = len(tokens[0]) - len(inputs.input_ids)\n","    time_taken = (end_time - start_time) / 1_000_000_000\n","    print(\"time_taken speculative:\",time_taken)\n","    print(tokenizer.decode(tokens[0]))\n","    print()\n","    print(f\"Latency (Speculative Sampling): {new_tokens/time_taken:.2f} tok/s\")\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T17:40:18.289195Z","iopub.status.busy":"2023-12-13T17:40:18.288527Z","iopub.status.idle":"2023-12-13T17:41:32.944550Z","shell.execute_reply":"2023-12-13T17:41:32.942665Z","shell.execute_reply.started":"2023-12-13T17:40:18.289150Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a5e7c5130de4dfd8f2fe4e0d25b6c23","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["time_taken autoregressive_sampling: 0.002318384\n","<s> Emily found a mysterious letter on her doorstep one sunny morning.\n","\n","Latency (Naive Autoregressive Sampling): 6038.69 tok/s\n"]}],"source":["import sys\n","import time\n","import random\n","import argparse\n","import torch\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n","print(device)\n","\n","method = \"Autoregressive\"\n","target_model_1 = \"mistralai/Mistral-7B-Instruct-v0.1\"\n","prompt = \"Emily found a mysterious letter on her doorstep one sunny morning.\"\n","max_new_tokens_1 = 12\n","temperature_1 = 1\n","\n","target_model = AutoModelForCausalLM.from_pretrained(target_model_1).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(target_model_1)\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","start_time = time.time_ns()\n","tokens = autoregressive_sampling(\n","    target_model,\n","    initial_prompt_seq=inputs.input_ids,\n","    target_len=max_new_tokens_1 + len(inputs.input_ids),\n","    temperature=temperature_1,\n",")\n","end_time = time.time_ns()\n","\n","new_tokens = len(tokens[0]) - len(inputs.input_ids)\n","time_taken = (end_time - start_time) / 1_000_000_000\n","\n","print(\"time_taken autoregressive_sampling:\", time_taken)\n","print(tokenizer.decode(tokens[0]))\n","print()\n","print(f\"Latency (Naive Autoregressive Sampling): {new_tokens/time_taken:.2f} tok/s\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
